{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c2dd92",
   "metadata": {},
   "source": [
    "## Part 1: Conceptual Warm-Up (No Coding)\n",
    "\n",
    "**Goal:** Ensure students understand the foundational concepts through explanation and derivation.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "#### 1. Short Answer Questions\n",
    "\n",
    "a) Derive the gradient update rule for the weight(s) for a single neuron with one input feature (i.e.,  \n",
    "field, or attribute) and the sigmoid activation function:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "b) Explain the role of the activation function in a neural network. Please guess without searching  \n",
    "textbook/the Internet.\n",
    "\n",
    "c) What is overfitting, and how can it be mitigated? List a couple of techniques to reduce overfitting.\n",
    "\n",
    "d) Can you guess what might be an underfitting problem? How can that be mitigated? List a couple of  \n",
    "techniques to reduce underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317c83a",
   "metadata": {},
   "source": [
    "**a.)**\n",
    "First we require the following inputs\n",
    "\n",
    "For the gradient rule update we need three main components:\n",
    "\n",
    "The Weighted Sum:\n",
    "$z = wx+b$\n",
    "\n",
    "Activation:\n",
    "$a = \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Cost:\n",
    "$C = (a-y)^2$\n",
    "\n",
    "**b.)**\n",
    "An activation function such as sigmoid or ReLu is used in neural networks to make sense of the weights and inputs in a given neuron of a neural network. In a Sigmoid it squishes the outputs into a value between 0 and 1 to help determine the magnitude of neuron in a network.\n",
    "\n",
    "**c.)**\n",
    "Overfitting is the problem of training a model too well on a dataset where it preforms incredibly well on the same dataset but then falls short outside of it, preforming worse than expected. Overfitting is largly caused by over trainning on the same data set and having too long. Ways to mitigate overfitting is to limit training time to not overtrain on the same data, provide different data sets (potentially instead of feeding one large dataset at once break it into pieces and train it on each set separately), and providing more data. Another alternative is to change the network structure like changing the number of weights and their values.\n",
    "\n",
    "**d.)**\n",
    "Underfitting would presumably be the opposite to overfitting where the model is not training at all to the dataset provided. My assumption would be that similar methods to rectify the issue would apply: provide more quality data, maybe the model has not had enough time to train, and also adjustments to the structure may be required. Further reading on the subject my assumptions are roughly correct, a reason of underfitting could be too simple of a data set to create meaningful connections and relationships in the network. Looks like eit it is also due to poor feature engineering or excessive regulation of a dataset, ommiting key datapoints of a data set which would cause poor trainning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2982f",
   "metadata": {},
   "source": [
    "#### 2. Math Problem\n",
    "\n",
    "a) For the following small binary classification dataset (4 samples each with two input features  \n",
    "$(x_1, x_2)$ and one target feature $(y)$):  \n",
    "\n",
    "Compute the forward passes and the gradients manually for a neural network with 1 hidden layer  \n",
    "(with 2 neurons) and 1 output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9daf523",
   "metadata": {},
   "source": [
    "## Part 2: Build a Neural Network from Scratch (Coding)\n",
    "\n",
    "**Goal:** Students implement a simple neural network without relying on deep learning libraries like TensorFlow or PyTorch.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "#### 3. Implement a neural network with:\n",
    "\n",
    "a) 1 input layer, 1 hidden layer (with 2 neurons), and 1 output layer.  \n",
    "\n",
    "b) Use a non-linear activation function (e.g., sigmoid, ReLU, tanh, etc.) for each of the neurons.  \n",
    "\n",
    "c) Do not forget to introduce the bias inputs for the two neural layers (i.e., hidden layer and output layer).  \n",
    "\n",
    "d) Consider **mean squared error (MSE)** as the loss/error function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff7a35",
   "metadata": {},
   "source": [
    "#### 4. Train it on the dataset:\n",
    "\n",
    "- Small dataset of 4 samples (denoted by rows in *Fig. 1*).  \n",
    "- Each sample has two input features $(x_1, x_2)$ and one target feature $(y)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba83866",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "- Implement **forward propagation** and the **backpropagation algorithm** manually.  \n",
    "  (Refer to the boilerplate code for placeholders where your implementations should go.)  \n",
    "- Train the network for a fixed number of **epochs (10,000)** and plot the **loss over time**.  \n",
    "\n",
    "### Specific Guidelines\n",
    "\n",
    "- Do **not** use any pre-built neural network libraries. Only use libraries for basic operations (e.g., NumPy).  \n",
    "- Write **detailed comments** in your code to explain each step of your implementation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e555ee",
   "metadata": {},
   "source": [
    "## Part 3: Experimentation and Analysis\n",
    "\n",
    "**Goal:** Encourage critical thinking.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "#### 5. Hyperparameter Tuning\n",
    "\n",
    "- Experiment with different **learning rates** and **hidden layer sizes**.  \n",
    "- Analyze how these changes impact the **convergence** and **performance** of your model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0f849",
   "metadata": {},
   "source": [
    "#### 6. Visualization\n",
    "\n",
    "- Plot **decision boundaries** after training your model.  \n",
    "- Provide insights into how the model separates the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d5484",
   "metadata": {},
   "source": [
    "## Part 4: Reflective Questions\n",
    "\n",
    "**Goal:** Make students reflect on the learning process.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "#### 7. Backpropagation Challenges\n",
    "- What challenges did you face in implementing backpropagation?  \n",
    "- How did you overcome them?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3e0d3",
   "metadata": {},
   "source": [
    "#### 8. Debugging\n",
    "- Explain the importance of **debugging** in neural network training.  \n",
    "- Provide one strategy that helped you debug effectively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d913c",
   "metadata": {},
   "source": [
    "#### 9. Activation Function Choice\n",
    "- Discuss how the training process would change if you used a different activation function  \n",
    "  (e.g., tanh instead of ReLU, or vice versa).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
