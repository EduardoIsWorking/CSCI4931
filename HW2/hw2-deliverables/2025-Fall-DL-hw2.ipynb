{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2\n",
    "* CSCI-4931 : Deep Learning\n",
    "* Fall 2025\n",
    "* Instructor: Ashis Kumer Biswas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Please assign your name, and declare collaborators (if any) to the variables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_name = \" \"\n",
    "collaborators = \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little Background about the problem\n",
    "\n",
    "`Customer churn`` occurs when customers stop doing business with a company, also known as customer attrition. It is also referred to as loss of clients or customers.\n",
    "\n",
    "You are given sensitive information of 9,000 of an European Bank, EBQ. Your task is to build an Artificial Neural Network (ANN) based on the dataset such that later the ANN model can predict correctly who is going to leave next. This predictive analysis is vital for the EBQ bank to revise their business strategy towards customer retention. What do you think?\n",
    "\n",
    "Anyway, you are recruited by the bank to do the data science. And, the head of the bank only trusts heads, i.e., brains…. I mean neural networks for making any decisions. And luckily you were in Dr. B’s class and you know something(?) about the ANN that you could successfully convince the head of the bank during the interview. He has put a lot of faith in you. Now, can you solve his problem?\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Please download the zip file, `hw2-deliverables.zip``. Unzip it in your workspace. Here below is the file hierarchy of \"hw2-deliverables/\" folder:\n",
    "\n",
    "```\n",
    "hw2-deliverables\n",
    "├── 2025-Fall-DL-hw2.ipynb\n",
    "├── dataset\n",
    "│   └── datasetX.csv\n",
    "├── figures\n",
    "│   ├── le.png\n",
    "│   ├── nn-1.png\n",
    "│   ├── nn-1.svg\n",
    "│   ├── nn-2.png\n",
    "│   ├── nn-2.svg\n",
    "│   ├── nn-3.png\n",
    "│   ├── nn-3.svg\n",
    "│   └── ohe.png\n",
    "└── saved_models\n",
    "```\n",
    "As you can see you will mostly be working with the `2025-Fall-DL-hw2.ipynb`, i.e., the jupyter notebook. The notebook accesses the dataset files: `dataset/datasetX.csv` containing few customer information and is labeled (i.e., the target column, `Exited` is present). Here below is a brief summary of the features you will find in the datasets:\n",
    "\n",
    "* `CustomerId`: a unique identifier for each customer within the dataset. These values are not ordered sequentially within the dataset, and are only used to identify a specific customer. It typically does not have any influence to whether a customer leaves the business.\n",
    "* `Surname`: A string used to identify the customer in the dataset. Surname may be distinct amidst all or most customers. Because of this, it most likely won't affect the target variable. \n",
    "* `CreditScore`: a numeric representation of the customer's individual fiscal credit score. Typically used to indicate eligibility for loans. Current credit scores use a range from 300 to 850, but the FICO auto score range uses 250-900. This feature likely determines retention rate of customers. \n",
    "* `Geography`: this feature contains a categorical string representing the name of a country the customer is from originally. \n",
    "* `Gender`: this feature contains a categorical string representing the gender of the customer (\"Male\"/\"Female\"). \n",
    "* `Age`: a numerical integer representation of a customer's age. Intuition suggests that older customers are likely to have higher retention than younger customers.\n",
    "* `Tenure`: a numerical integer representation. It is assumed that this feature represents the number of total years the customer has been retained. It is likely that customers which have been retained longer will continue to be retained.\n",
    "* `Balance`: a numerical floating point number (to two decimal places of precision) indicating the customer's current bank balance (assumed total across all accounts). Customers with a greater balance may be less likely to exit the account due to difficulty of transfer. \n",
    "* `NumOfProducts`: numeric integer value. It is assumed that this value represents the number of accounts (products) that this customer has open. Further evaluation of this feature would be needed to determine the usefulness of this feature, but at face-value, intuition dictates that a customer with more products is less likely to exit. \n",
    "* `HasCrCard`: boolean flag (0 or 1) representing whether the customer has a credit card or not. \n",
    "* `IsActiveMember`: boolean flag (0 or 1) representing whether the customer is an active member of the bank. It is assumed this indicates whether the customer has transactions on the regular banking statement. Intuition dictates that inactive members are more likely to exit. \n",
    "* `EstimatedSalary`: numerical floating point representation of the customer's predicted salary (to two decomal places) intuition dictates that customers with different incomes may behave differently with respect to retention rate. \n",
    "* `Exited`: boolean flag (0 or 1) representing whether the customer has exited their account. This is the target variable for the dataset. It should not be dropped, but should not be included as the training input (X), and should instead be separated as the target label (y). \n",
    "\n",
    "You will also see an empty directory `saved_models/`, that is for you to save all the models you'd train in this assignment.\n",
    "\n",
    "`figures/` directory contains few image files used to properly document this assignment. Please do not delete and when possible please move them with this jupyter notebook for proper display of its contents.\n",
    "\n",
    "> In this Jupyter notebook please write your solutions / codes in the cells marked with `#Your solution goes here...`. You may add additional code cells after that cell if you desire. But, please do not remove any cell originally given in the notebook.\n",
    "\n",
    "> After you solve the assignment in the jupyter notebook, be sure to execute and save it so that execution/results/printouts are also saved with it.\n",
    "> Finally, submit the saved jupyter notebook (`2025-Fall-DL-hw2.ipynb`) in Canvas to receive grade. Optionally, you can also submit a python version of the notebook if desired. For this assignment, Canvas only will accept either jupyter notebook in `*.ipynb` or python script (`*.py`) extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : (10 points)\n",
    "* Define a function named `summarize_dataset` that takes only one argument: `csv_file`, where `csv_file` is the name of the given `csv` file with this assignment, i.e., `datasetX.csv`. \n",
    "  * The function is expected to summarize the given dataset in the following way:\n",
    "```\n",
    "total number of rows = a\n",
    "total number of columns = b\n",
    "number of columns having non-numeric values = c\n",
    "columns with missing values = [ (d1, e1)  (d2, e2), ... ]\n",
    "gender based summary of exited column = [ (f1, g1)  (f2, g2), ... ]\n",
    "age based summary of exited column = [ ('below or equal to 40', h1)  ('above 40', h2) ]\n",
    "credit score summary =  i +/- j \n",
    "```\n",
    "  \n",
    "where,\n",
    "\n",
    "* `a` is total number of rows in the dataset.\n",
    "* `b` is total number of columns in the dataset.\n",
    "* `c` is number of columns having non-numeric values.\n",
    "* $(d_i, e_i)$ (i.e., a pair/tuple entry) represents column name ($d_i$) and number of missing values present in that column ($e_i$). If number of missing values in a column is zero (0), you do not need to list it. Please sort the tuple entries in descending order of $e_i$ values.\n",
    "* $g_i$ represents the percentage of gender $f_i$ who exited. Please sort the tuple entries entries in descending order of $g_i$ values. Also, print the percentages in 2 decimal places after the decimal point, and print use `%` symbol after the percentage value.\n",
    "* $h_1$ and $h_2$ represents the percentage of $\\leq 40$ year olds who exited, and the percentage $>40$ year olds who exited.  Also, print the percentages in 2 decimal places after the decimal point, and print use `%` symbol after the percentage value.\n",
    "* `j` and `k` are average and standard deviation of credit scores among the data samples respectively. Please print the way it is shown above. Also, print the both values in 2 decimal places after the decimal point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution goes here...\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/datasetX.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "a = df.shape[0] # number of rows\n",
    "print(f\"{a}\")\n",
    "\n",
    "b = df.shape[1] # number of columns\n",
    "print(f\"{b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15647572</td>\n",
       "      <td>Greece</td>\n",
       "      <td>504.0</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>54980.81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>136909.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15797692</td>\n",
       "      <td>Volkova</td>\n",
       "      <td>659.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7</td>\n",
       "      <td>89939.62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>136540.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15713559</td>\n",
       "      <td>Onyemauchechukwu</td>\n",
       "      <td>473.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>32.0</td>\n",
       "      <td>5</td>\n",
       "      <td>146602.25</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72946.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15595067</td>\n",
       "      <td>Zhirov</td>\n",
       "      <td>637.0</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>181610.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15810167</td>\n",
       "      <td>Scott</td>\n",
       "      <td>657.0</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>75.0</td>\n",
       "      <td>7</td>\n",
       "      <td>126273.95</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>91673.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>15770214</td>\n",
       "      <td>Bryant</td>\n",
       "      <td>754.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144134.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>15720134</td>\n",
       "      <td>Reynolds</td>\n",
       "      <td>709.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9</td>\n",
       "      <td>115479.48</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>134732.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>15700549</td>\n",
       "      <td>Alvares</td>\n",
       "      <td>721.0</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4493.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>15704081</td>\n",
       "      <td>Findlay</td>\n",
       "      <td>595.0</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9</td>\n",
       "      <td>130682.11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57862.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>15767729</td>\n",
       "      <td>Smith</td>\n",
       "      <td>646.0</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5</td>\n",
       "      <td>182876.88</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42537.59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CustomerId           Surname  CreditScore Geography  Gender   Age  \\\n",
       "0       15647572            Greece        504.0     Spain    Male   NaN   \n",
       "1       15797692           Volkova        659.0    France  Female  33.0   \n",
       "2       15713559  Onyemauchechukwu        473.0   Germany  Female  32.0   \n",
       "3       15595067            Zhirov        637.0     Spain  Female  40.0   \n",
       "4       15810167             Scott        657.0     Spain    Male  75.0   \n",
       "...          ...               ...          ...       ...     ...   ...   \n",
       "8995    15770214            Bryant        754.0    France  Female  27.0   \n",
       "8996    15720134          Reynolds        709.0   Germany    Male  30.0   \n",
       "8997    15700549           Alvares        721.0    France    Male  54.0   \n",
       "8998    15704081           Findlay        595.0   Germany    Male  30.0   \n",
       "8999    15767729             Smith        646.0     Spain    Male  25.0   \n",
       "\n",
       "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0          0   54980.81              1          1               1   \n",
       "1          7   89939.62              1          1               0   \n",
       "2          5  146602.25              2          1               1   \n",
       "3          6       0.00              2          1               1   \n",
       "4          7  126273.95              1          0               1   \n",
       "...      ...        ...            ...        ...             ...   \n",
       "8995       7       0.00              2          1               0   \n",
       "8996       9  115479.48              2          1               1   \n",
       "8997       5       0.00              2          1               1   \n",
       "8998       9  130682.11              2          1               1   \n",
       "8999       5  182876.88              2          1               1   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           136909.88       0  \n",
       "1           136540.09       0  \n",
       "2            72946.95       0  \n",
       "3           181610.60       0  \n",
       "4            91673.60       0  \n",
       "...               ...     ...  \n",
       "8995        144134.64       0  \n",
       "8996        134732.99       0  \n",
       "8997          4493.12       0  \n",
       "8998         57862.88       0  \n",
       "8999         42537.59       1  \n",
       "\n",
       "[9000 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset (csv_file):\n",
    "    my_df = pd.read_csv(csv_file)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dataset('datasetX.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "* Preprocessing the given dataset for the model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (10 points)\n",
    "\n",
    "* First preprocessing that we are going to do on the dataset is dropping two features (i.e., columns) that, I think, are irrelevant and would not make any meaningful relationship with the `Exited` feature. The features are: `CustomerId` and `Surname`.\n",
    "* Make sure to create a variable called `dataset_dropped` that will store the revised dataset.\n",
    "* Please print the name of the columns of the revised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-247a930d79bde8e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 (10 points)\n",
    "* Second Preprocessing that we are going to do is *Shuffle Rows* of the dataset obtained from `Task 2.1`.\n",
    "* \"It is extremely important to shuffle the training data, so that you do not obtain entire minibatches of highly correlated examples. As long as the data has been shuffled, everything should work OK. Different random orderings will perform slightly differently from each other but this will be a small factor that does not matter much.\" -- [Ian Goodfellow](https://qr.ae/pGBgw8)\n",
    "* Use a random seed value `4321` in case you will call any stochastic method.\n",
    "* Make sure to create a variable called `dataset_shuffled` that will store the revised dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "shuff_dataset_final",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: (10 points)\n",
    "\n",
    "* Third Preprocessing that we will do is X-y Partitioning of the dataset obtained from `Task 2.2`.\n",
    "* In its current state, the dataset contains both independent (input, `X`) and the target (output, `y`) features within the same dataframe. For ease of of the training process, we need to partition the training features from the target feature into two separate dataframes. \n",
    "* Make sure, the following cell contains at least two variables: `X` and `y`:\n",
    "  * `X` contains part of the dataset with only independent features, and \n",
    "  * `y` having only the dependent/target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution goes here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 (10 points)\n",
    "* Fourth Preprocessing that we will do is Train-Test Split of X, y obtained from `Task 2.3`.\n",
    "* Now that we have X and y tables with appropriate feature pruning performed, we must split the data into a training partition (`X_train, y_train`) and a testing partition (`X_test, y_test`). \n",
    "* The training partitions (`X_train, y_train`) will be used to train your model, while the test partition (`X_test, y_test`) will be set aside during the training steps, and will only be used to evaluate the trained model. \n",
    "* Training and test splits should be mutually exclusive to the datasets... i.e., a sample can not be both in training and test sets.\n",
    "* Please perform a 80-20 split, meaning 80% of the (X,y) dataset will be in (X_train, y_train) split, while, remaining 20% will be in (X_test,y_test) split. \n",
    "* Please use random seed `4321` prior to calling any stochastic methods.\n",
    "* Make sure the following cell contains at least 4 variables: `X_train`, `y_train`, `X_test`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "train_test_split",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 (10 points)\n",
    "\n",
    "* Fifth preprocessing that we will do is the *Conversion of Categorical features to Numerical*\n",
    "* Please adopt the `One Hot Encoding` method instead of `Label Encoding` while converting the categorical features. \n",
    "* Make sure the following cell contains a variable named `X_train_ohe` that would contain one hot encoded `X_train` data; on the two categorical columns: 'Geography','Gender'. Please save the encoder for later use; e.g., encode `X_test` dataset, or any future test sample given to you. Under any circumstance, you must not encode `X_test` independently like you would do for `X_train`.\n",
    "* Now, encode the `X_test` data using the one hot encoder you saved while you encoded the `X_train`, and name the variable `X_test_ohe`.\n",
    "\n",
    "\n",
    "* **Both encoding techniques are outlined below**:\n",
    "> A little background first: Categorical features are features that contain values that are not numeric. It would be absurd to work with non-numeric features if you ask neurons in your ANN to compute the weighted sum of inputs, and then pass through activation function, right? These maths are undefined. An obvious solution you may be intrigued to do is dropping the features! Aha! Wrong!! Every piece of data is precious... may present with valuable insights of the data samples to find the patterns to map inputs with output/targets. So, we should include them. But, how?\n",
    "\n",
    "The answer is via \"Encoding\". \n",
    "\n",
    "Several types of encodings are used in practice. Here below are just 2 popular ones:\n",
    "1. **Label Encoding**, where labels are encoded as subsequent numbers. Say, for a categorical feature named \"Category\" with three categorical values: {“Cat”, “Dog” or “Zebra”} can be encoded to \"0\", \"1\", \"2\" respectively as in figure below. The issue with this type of encoding may unintentionally impose a type of ordering of the categories, that may add bias to the training.\n",
    "\n",
    "\n",
    "![label-encoding](figures/le.png)\n",
    "\n",
    "2. **One Hot Encoding**, ignores the ordering of the categories all together. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1. Also, don't forget to remove the original categorical features. Here below just an example, how to convert the categorical feature called \"Category\" having the {“Cat”, “Dog” or “Zebra”} values into three new binary features: \"Cat\", \"Dog\", \"Zebra\".\n",
    "\n",
    "![label-encoding](figures/ohe.png)\n",
    "\n",
    "**A note on the Dummy Variable Trap**\n",
    "The Dummy Variable Trap occurs when two or more dummy variables created by one-hot encoding are highly correlated (i.e., becomes multi-collinear). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models. In other words, the individual effect of the dummy variables on the prediction model can not be interpreted well because of multicollinearity.\n",
    "\n",
    "Using the one-hot encoding method, a new dummy variable is created for each categorical variable to represent the presence (1) or absence (0) of the categorical variable. For example, if tree species is a categorical variable made up of the values pine, or oak, then tree species can be represented as a dummy variable by converting each variable to a one-hot vector. This means that a separate column is obtained for each category, where the first column represents if the tree is pine and the second column represents if the tree is oak. Each column will contain a 0 or 1 if the tree in question is of the column's species. These two columns are multi-collinear since if a tree is pine, then we know it's not oak and vice versa. The machine learning models trained on dataset having this multi-collinearity suffers. A remedy is to drop first (or any one) of the dummy (i.e., one-hot) features created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "one-hot-encoding-function",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: (10 points)\n",
    "\n",
    "* Sixth Preprocessing that we are going to do is *Normalization of X_train_ohe, and X_test_ohe*\n",
    "\n",
    "* Now that we have all numerical training and test datasets: `X_train_ohe` and `X_test_ohe` respectively, we can normalize each features in both of the datasets. **Normalization** is just one of the way to scale each feature. In class you'll learn a ton of other ways to scale. For this task, let's resort to **Normalization**.\n",
    "\n",
    "> \"The rule of thumb for scaling datasets, is we scale training dataset first, then using the statistics that we learn during the scaling process, we scale the test dataset. We do not learn any new statistics while we scale the test dataset.\"\n",
    "\n",
    "* Also, scaling is commonly performed column-wise, and never sample/row wise.\n",
    "\n",
    "* Make sure the following cell contains the two scaled variables: `X_train_scaled` and `X_test_scaled` based on the requirements mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "normalizer-learning-training",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your solution goes here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: (10 points)\n",
    "* *Designing your first Artificial Neural Network (ANN) based classifier* using **PyTorch**.:\n",
    "\n",
    "\n",
    "### Step 0: Dataset and DataLoaders\n",
    "* Make sure you implement the `Dataset` and `DataLoader` classes ready to read from the external csv file located at `dataset/datasetX.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: The ANN architecture\n",
    "* Let's design the first artificial network architecture for the classifier we would like to build. Here below is one. How did I get this architecture? Maybe in my dream! Haha. Someday you will get one too. Until that, let's follow the architecture below:\n",
    "  ![Task 3 ANN architecture](figures/nn-1.png)\n",
    "  * **Input layer** will have 11 units as the dimension of training set: `X_train_scaled` (i.e, number of columns = 11).\n",
    "  * **First hidden layer** will have 5 neurons, each with \"Rectified Linear Unit (`ReLU``)\" as activation function.\n",
    "  * **Second hidden layer** will have 4 neurons, each with \"`ReLU`\" as activation function.\n",
    "  * **Output layer** will have just 1 neuron, with `sigmoid`` activation function. \n",
    "    * The reason behind a single neuron with `sigmoid` activation at the output layer is that, output of this neuron will tell the probability score of the target outcome: \"Exited\" True or False. If the output neuron produces value above 0.5, we will say the neural network predicted \"True\", otherwise, False. This is the beauty of using sigmoid function at the output layer as we can interpret the output value of the neuron as probability score.\n",
    "* The architecture will come to life when you initiate the training process with training data.\n",
    "  * The training process needs a **gradient descend based optimizer**, and a convex looking **loss function**.  \n",
    "  * For this task, let's choose the `adam` optimizer, and the `binary_crossentropy` as the loss function.\n",
    "  * You choose a batch size that would make the execution comfortable at your workstation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: The Training process\n",
    "* Let's start the training process with the training dataset, `X_train_scaled`.\n",
    "  * Gradient descend based optimization updates run in iterations. When number of iterations equal the total number of training samples, we call that `1 epoch` has passed. Let's continue the training for `25 epochs`. But, you are welcome to run longer than this. There are, however, simpler way to determine if you should early stop your training. \n",
    "    * (Optional) Can you extract information about optimization in each epoch? If so, draw a epoch-loss plot, where X-axis needs to show epoch numbers, and Y-axis will show the `binary_crossentropy` loss value in that particular epoch iteration.\n",
    "* Don't forget to save the model into a file in the `saved_models/` directory so that you can re-use it later for further prediction. Let's give it a name: `model-ann-11-5-4-1-pt` with an extension of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Evaluation\n",
    "\n",
    "#### (part 3.1) Evaluating your model with the entire test dataset:\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-pt` from the file, and have it predict the entire test set you have at head: (`X_test_scaled`). Luckily, for each of the test sample in the set, you also have ground true `Exited` value in the `y_test`. \n",
    "* Please report/print your model's predictive performance on the test set in terms of `accuracy`, `precision`, `recall`, and `F1 scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (part 3.2) Evaluating your model with 1 test sample with known Exited value\n",
    "\n",
    "* Here is a single test sample for which we know the ground true `Exited` value:\n",
    "\n",
    "| CustomerId | Surname | CreditScore | Geography | Gender | Age | Tenure | Balance | NumOfProducts | HasCrCard | IsActiveMember |EstimatedSalary | Exited |\n",
    "| :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: |          ---: |\n",
    "| 55443322 | Reynolds |709|Germany|Male|30|9|115479.48|2|1|1|134732.99|0|\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-pt` from the file, and have it predict the test sample above. Please don't forget to preprocess this test samples so that it is compliant with the input and model requirements.\n",
    "* Please report whether it predicts a 0 or 1 for the `Exited` target, and also comment whether your model makes a mistake or predicts correctly.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution goes here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (part 3.3) Evaluating your model with 1 test sample without known Exited value\n",
    "\n",
    "* Here is a single test sample for which we **do not know** the ground true `Exited` value:\n",
    "\n",
    "| CustomerId | Surname | CreditScore | Geography | Gender | Age | Tenure | Balance | NumOfProducts | HasCrCard | IsActiveMember |EstimatedSalary | \n",
    "| :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: | :---        |    :----:   |          ---: |\n",
    "| 55443323 | Nguyen |603|France|Female|76|20|123456.78|5|1|1|55000.00|\n",
    "\n",
    "\n",
    "* Load your trained model `model-ann-11-5-4-1-pt` from the file, and have it predict the test sample above. Please don't forget to preprocess this test samples so that it is compliant with the input and model requirements.\n",
    "* Please report whether it predicts a 0 or 1 for the `Exited` target. Can you comment on this data sample whether your model captured the pattern in the population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ANN-1_layers",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Your solution goes here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: (10 points)\n",
    "\n",
    "* Repeat all the steps in Task 3 with the following new architecture of the neural network:\n",
    "\n",
    "![Task 4 ANN architecture](figures/nn-2.png)\n",
    "\n",
    "* Input layer will still have 11 units as the dimension of training set (i.e, number of columns = 11).\n",
    "* Hidden-layer-1: 8 neurons, with relu activation\n",
    "* Hidden-layer-2: 8 neurons, with relu activation,\n",
    "* Hidden-layer-3: 8 neurons, with relu activation,\n",
    "* Output-layer: 1 neuron with sigmoid.\n",
    "\n",
    "* You may re-use the Dataset/Dataloader classes you defined in Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution goes here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: (10 points)\n",
    "\n",
    "* Again, repeat Task 3 with the following new architecture of the neural network:\n",
    "\n",
    "![Task 5 ANN architecture](figures/nn-3.png)\n",
    "\n",
    "* Input layer will still have 11 units as the dimension of training set (i.e, number of columns = 11).\n",
    "* Hidden-layer-1: 8 neurons, with relu activation\n",
    "* Hidden-layer-2: 4 neurons, with relu activation,\n",
    "* Hidden-layer-3: 2 neurons, with relu activation,\n",
    "* Output-layer: 1 neuron with sigmoid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your solution goes here ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all folks! Thanks for your effort. \n",
    "\n",
    "Now, do the following to earn credit --\n",
    "\n",
    "0. Setting up:\n",
    "    - Make sure you actually experiment with this assignment. I would encourage (again) you to go through the possible compute resource you can use (e.g., Kaggle, Google Colab, etc.).\n",
    "    - It's always better to work in python virtual environment. Here are some resources for you to create and work in virtual environments [[win+mac+ubuntu](https://ashiskb.info/posts/2022/09/biswas/blog-1-python-venv/)][[windows+gpu](https://ashiskb.info/posts/2023/08/biswas/blog-win10-tensorflow/)][[ubuntu+gpu](https://ashiskb.info/posts/2023/08/biswas/blog-ubuntu-tensorflow/)]\n",
    "1. Please make sure to execute each cell in this jupyter notebook, and hit the 'Save' button, or go \"File > Save and Checkpoint\" menu option to save the notebook.\n",
    "2. Submit this notebook \"2025-Fall-DL-hw2.ipynb\" into Canvas \"hw-2\" entry. \n",
    "3. Done!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
