{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6515ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f35d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5da9eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Data generators ----------\n",
    "def make_odd_sequences(num_seqs=2000, min_len=5, max_len=12):\n",
    "    # Each sequence is a run of odd numbers with random start (odd) and small length\n",
    "    # e.g., 1,3,5,7,9, ...  or 5,7,9,11,13, ...\n",
    "    seqs = []\n",
    "    for _ in range(num_seqs):\n",
    "        L = random.randint(min_len, max_len)\n",
    "        start = random.choice([1,3,5,7,9,11,13])\n",
    "        seq = [start + 2*i for i in range(L)]\n",
    "        seqs.append(seq)\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a749305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fib_sequences(num_seqs=2000, min_len=6, max_len=14):\n",
    "    # Fibonacci-like with random small seeds (avoid exploding magnitudes)\n",
    "    # e.g., a0=1, a1=1 -> 1,1,2,3,5,8,...\n",
    "    # or random tiny seeds like (1,2) -> 1,2,3,5,8,...\n",
    "    seqs = []\n",
    "    for _ in range(num_seqs):\n",
    "        L = random.randint(min_len, max_len)\n",
    "        a0 = random.randint(0,2)\n",
    "        a1 = random.randint(1,3)\n",
    "        seq = [a0, a1]\n",
    "        for _ in range(L-2):\n",
    "            seq.append(seq[-1] + seq[-2])\n",
    "        seqs.append(seq)\n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0af1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_batches(seqs, in_len=5):\n",
    "    \"\"\"\n",
    "    Create many (input_seq -> next_value) training examples.\n",
    "    Inputs shape: [batch, time, 1], Targets shape: [batch, time, 1]\n",
    "    For a sequence s[0..L-1], we create sliding windows:\n",
    "      x: s[0..in_len-1] -> y: s[1..in_len]\n",
    "      x: s[1..in_len]   -> y: s[2..in_len+1]\n",
    "      ...\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for s in seqs:\n",
    "        # simple normalization to stabilize training\n",
    "        scale = max(abs(v) for v in s) if max(abs(v) for v in s) > 0 else 1.0\n",
    "        ns = [v/scale for v in s]\n",
    "        for start in range(0, len(ns) - in_len):\n",
    "            x = ns[start:start+in_len]\n",
    "            y = ns[start+1:start+in_len+1]\n",
    "            X.append([[xi] for xi in x])  # time x 1\n",
    "            Y.append([[yi] for yi in y])  # time x 1\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32)\n",
    "    return X, Y  # [N, T, 1], [N, T, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b53d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=16, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # We'll implement using nn.RNN (tanh) to keep it \"vanilla\"\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, nonlinearity='tanh', batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # x: [B,T,1]\n",
    "        out, hT = self.rnn(x, h0)           # out: [B,T,H]\n",
    "        y = self.fc(out)                    # [B,T,1]\n",
    "        return y, hT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d76b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, hidden_size=16, epochs=8, lr=1e-2, batch_size=128):\n",
    "    ds = torch.utils.data.TensorDataset(X, Y)\n",
    "    dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = VanillaRNN(1, hidden_size, 1).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for ep in range(1, epochs+1):\n",
    "        total = 0.0\n",
    "        for xb, yb in dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            yhat, _ = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item() * xb.size(0)\n",
    "        print(f\"Epoch {ep:02d} | Loss {total/len(ds):.6f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec94fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sequence(model, seed, steps=10):\n",
    "    \"\"\"\n",
    "    seed: list of numbers (un-normalized). We auto-normalize internally\n",
    "    so generation stays numerically stable; then de-normalize at the end.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    scale = max(1.0, max(abs(v) for v in seed))\n",
    "    hist = [v/scale for v in seed]        # normalized working list\n",
    "    in_len = len(hist)\n",
    "\n",
    "    x = torch.tensor([[ [v] for v in hist ]], dtype=torch.float32).to(device)  # [1,T,1]\n",
    "    _, h = model.rnn(x)  # prime hidden state with the seed\n",
    "\n",
    "    last = hist[-1]\n",
    "    out_vals = seed[:]   # store de-normalized values for return\n",
    "\n",
    "    for _ in range(steps):\n",
    "        inp = torch.tensor([[[last]]], dtype=torch.float32).to(device)  # [1,1,1]\n",
    "        y, h = model(inp, h)               # one-step advance\n",
    "        pred = y[0,0,0].item()             # normalized prediction\n",
    "        denorm = pred * scale\n",
    "        out_vals.append(denorm)\n",
    "        last = pred\n",
    "\n",
    "    return out_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f14aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss 0.067857\n",
      "Epoch 02 | Loss 0.004303\n",
      "Epoch 03 | Loss 0.001816\n",
      "Epoch 04 | Loss 0.001191\n",
      "Epoch 05 | Loss 0.000855\n",
      "Epoch 06 | Loss 0.000676\n",
      "Epoch 07 | Loss 0.000551\n",
      "Epoch 08 | Loss 0.000471\n"
     ]
    }
   ],
   "source": [
    "# Training first sequence generator\n",
    "odd_seqs = make_odd_sequences()\n",
    "Xo, Yo = to_batches(odd_seqs, in_len=5)\n",
    "odd_model = train_model(Xo, Yo, hidden_size=16, epochs=8, lr=1e-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eef869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Odd numbers generation from seed [1,3,5,7,9]:\n",
      "[1, 3, 5, 7, 9, 9.394219279289246, 9.634800553321838, 9.856371402740479, 10.010874152183533, 10.122614979743958, 10.216217637062073, 10.287319779396057, 10.342536807060242]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOdd numbers generation from seed [1,3,5,7,9]:\")\n",
    "print(generate_sequence(odd_model, seed=[1,3,5,7,9], steps=8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0628b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss 0.008856\n",
      "Epoch 02 | Loss 0.000201\n",
      "Epoch 03 | Loss 0.000158\n",
      "Epoch 04 | Loss 0.000128\n",
      "Epoch 05 | Loss 0.000110\n",
      "Epoch 06 | Loss 0.000100\n",
      "Epoch 07 | Loss 0.000093\n",
      "Epoch 08 | Loss 0.000086\n",
      "Epoch 09 | Loss 0.000081\n",
      "Epoch 10 | Loss 0.000076\n",
      "Epoch 11 | Loss 0.000072\n",
      "Epoch 12 | Loss 0.000069\n"
     ]
    }
   ],
   "source": [
    "# Training the 2nd sequence generator\n",
    "fib_seqs = make_fib_sequences()\n",
    "Xf, Yf = to_batches(fib_seqs, in_len=6)\n",
    "fib_model = train_model(Xf, Yf, hidden_size=32, epochs=12, lr=5e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fibonacci-like generation from seed [1,1,2,3,5,8]:\n",
      "[5, 8, 13, 21, 34, 62.98739004135132, 84.00771617889404, 105.28031635284424, 116.44433212280273, 121.62806463241577, 123.39754962921143, 124.0987548828125, 124.3146562576294]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFibonacci-like generation from seed [1,1,2,3,5,8]:\")\n",
    "print(generate_sequence(fib_model, seed=[1,1,2,3,5,8], steps=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46b43e",
   "metadata": {},
   "source": [
    "Q1: Carefully observe the two Vanilla RNN models. Please comment on its strength and weaknesses.\n",
    "Q2: How are the two Vanilla RNN models different than a standard feedforward Neural Network? Can we do it with a feedforward NN?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
