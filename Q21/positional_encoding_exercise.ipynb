{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec220d12",
   "metadata": {},
   "source": [
    "# Programming Exercise: Positional Encoding in Transformers\n",
    "\n",
    "**Duration:** 45 minutes\n",
    "**Goals:** Implement sinusoidal positional encodings, visualize them, compare to learned embeddings, and study how they affect attention.\n",
    "\n",
    "> Complete all TODOs and answer short prompts in the designated Markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86783710",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this exercise, you will be able to:\n",
    "1. Implement **sinusoidal positional encodings** from *Attention Is All You Need*.\n",
    "2. Visualize how different embedding dimensions oscillate at different frequencies.\n",
    "3. Explain how positional encodings provide **absolute** and **relative** position cues.\n",
    "4. Compare **fixed (sinusoidal)** vs **learned** positional embeddings with a tiny experiment.\n",
    "5. Observe how adding positional encodings changes **dot‑product attention** patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup ===\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# IMPORTANT FOR THIS EXERCISE:\n",
    "# - Use matplotlib only (no seaborn).\n",
    "# - Keep each chart in its own figure (no subplots).\n",
    "# - Do NOT set specific colors or styles unless asked.\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e8a4b",
   "metadata": {},
   "source": [
    "## Part 1 — Implement sinusoidal positional encodings (Required)\n",
    "\n",
    "**Task:** Implement the function `sinusoidal_pe(max_len, d_model)` that returns a NumPy array of shape `(max_len, d_model)` using:\n",
    "\n",
    "$$PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "**Checklist:**\n",
    "- Works for any `max_len ≥ 1` and `d_model ≥ 2`.\n",
    "- Even dims use `sin`, odd dims use `cos`.\n",
    "- Vectorized implementation (no slow Python loops over positions is preferred but not required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_pe(max_len: int, d_model: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute sinusoidal positional encodings.\n",
    "    Returns: array of shape (max_len, d_model)\n",
    "    \"\"\"\n",
    "    # TODO: Implement per the formula above.\n",
    "    # Hints:\n",
    "    # - Create an array of positions shape (max_len, 1)\n",
    "    # - Create a dim index array shape (1, d_model)\n",
    "    # - Compute angle rates with 10000 ** (2 * (i//2) / d_model)\n",
    "    # - Apply sin to even columns, cos to odd columns\n",
    "    raise NotImplementedError('Implement sinusoidal_pe(max_len, d_model)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e158a40",
   "metadata": {},
   "source": [
    "### Quick checks\n",
    "\n",
    "Run the following cell after your implementation. It performs simple structural checks and prints a preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sanity checks (do not modify) ===\n",
    "def _quick_checks():\n",
    "    pe = sinusoidal_pe(8, 6)\n",
    "    assert pe.shape == (8, 6), 'Shape should be (max_len, d_model) == (8, 6)'\n",
    "    # position 0 should be [0,1,0,1,...]\n",
    "    row0 = pe[0]\n",
    "    np.testing.assert_allclose(row0[0::2], np.zeros(3), atol=1e-7)\n",
    "    np.testing.assert_allclose(row0[1::2], np.ones(3), atol=1e-7)\n",
    "    print('Basic checks passed. Preview:\\n', np.round(pe[:5], 4))\n",
    "\n",
    "try:\n",
    "    _quick_checks()\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e187a2c",
   "metadata": {},
   "source": [
    "## Part 2 — Visualize positional encodings (Required)\n",
    "\n",
    "**Task:** Visualize the heatmap for `d_model = 6` and positions `0..49` using your function.\n",
    "\n",
    "**Prompt:** In the Markdown cell below the plot, explain:\n",
    "- Which dimensions oscillate fastest vs slowest?\n",
    "- What does this imply for capturing local vs global position information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9778d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Your visualization ===\n",
    "max_len = 50\n",
    "d_model = 6\n",
    "pe = sinusoidal_pe(max_len, d_model)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(pe, aspect='auto')\n",
    "plt.colorbar(label='value')\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Position (token index)')\n",
    "plt.title('Positional Encoding Heatmap (sinusoidal)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9f7eb",
   "metadata": {},
   "source": [
    "**Your explanation (2–4 sentences):**  \n",
    "*Write here…*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d700414",
   "metadata": {},
   "source": [
    "## Part 3 — Relative distance cue (Required)\n",
    "\n",
    "**Task:** Show that **relative positions** are encoded:  \n",
    "Pick two positions `p` and `q`, compute the cosine similarity between their encodings `PE[p]` and `PE[q]` **as a function of |p−q|**.\n",
    "\n",
    "1. Write a function `pairwise_cosine(pe)` that returns a `(max_len, max_len)` matrix of cosine similarities.\n",
    "2. Plot the matrix as an image for `max_len = 60, d_model = 32`.\n",
    "3. In the Markdown cell: Describe the visible pattern (e.g., bands/diagonals) and connect it to relative distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91071f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_cosine(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns cosine-similarity matrix of x.\n",
    "    x: (N, D)\n",
    "    \"\"\"\n",
    "    # TODO: Implement cosine similarity using normalization and matrix multiply.\n",
    "    raise NotImplementedError('Implement pairwise_cosine')\n",
    "\n",
    "# === Use your function ===\n",
    "max_len = 60\n",
    "d_model = 32\n",
    "pe = sinusoidal_pe(max_len, d_model)\n",
    "\n",
    "# Compute pairwise cosine similarities\n",
    "C = pairwise_cosine(pe)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(C, aspect='auto', vmin=-1.0, vmax=1.0)\n",
    "plt.colorbar(label='cosine')\n",
    "plt.xlabel('Position j')\n",
    "plt.ylabel('Position i')\n",
    "plt.title('Cosine Similarity between PE[i] and PE[j]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12657590",
   "metadata": {},
   "source": [
    "**Your explanation (3–5 sentences):**  \n",
    "*Write here…*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65252d6",
   "metadata": {},
   "source": [
    "## Part 4 — Learned vs Fixed Positional Embeddings (Optional but recommended)\n",
    "\n",
    "We’ll do a toy extrapolation test: learn a linear regressor to predict a simple target from **position** only.\n",
    "\n",
    "**Targets:**\n",
    "- `y(pos) = sin(pos / 3) + 0.3 * cos(pos / 11)` (a mixture of waves).\n",
    "\n",
    "**Train:** positions `0..39`.  \n",
    "**Test:** positions `40..79` (out-of-range).\n",
    "\n",
    "Compare two input representations:\n",
    "1. **Fixed sinusoidal PE**: feed `PE[pos]` to the regressor.  \n",
    "2. **One‑hot absolute position** (length = 40): feed one‑hot(pos) for training positions; at test time, zeros (cannot represent unseen positions).\n",
    "\n",
    "**Expectations:** Sinusoidal features extrapolate; one‑hot cannot.\n",
    "\n",
    "> Implement the experiment and plot train/test predictions. Use only `numpy` (no deep learning frameworks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Your small experiment ===\n",
    "\n",
    "def make_targets(positions):\n",
    "    return np.sin(positions / 3.0) + 0.3 * np.cos(positions / 11.0)\n",
    "\n",
    "# Data\n",
    "train_pos = np.arange(0, 40)\n",
    "test_pos  = np.arange(40, 80)\n",
    "\n",
    "# Inputs for model A: sinusoidal PE (use d_model=32 for richer features)\n",
    "d_model = 32\n",
    "PE_all = sinusoidal_pe(80, d_model)\n",
    "X_train_A = PE_all[train_pos]\n",
    "X_test_A  = PE_all[test_pos]\n",
    "\n",
    "# Inputs for model B: one-hot absolute positions (length 40)\n",
    "X_train_B = np.eye(40)[train_pos]\n",
    "X_test_B  = np.zeros_like(np.eye(40)[:len(test_pos)])  # cannot represent unseen positions\n",
    "\n",
    "y_train = make_targets(train_pos)\n",
    "y_test  = make_targets(test_pos)\n",
    "\n",
    "# Fit linear least squares: w = (X^T X)^(-1) X^T y\n",
    "def fit_linear(X, y):\n",
    "    return np.linalg.pinv(X) @ y\n",
    "\n",
    "w_A = fit_linear(X_train_A, y_train)\n",
    "w_B = fit_linear(X_train_B, y_train)\n",
    "\n",
    "y_pred_A_train = X_train_A @ w_A\n",
    "y_pred_A_test  = X_test_A  @ w_A\n",
    "\n",
    "y_pred_B_train = X_train_B @ w_B\n",
    "y_pred_B_test  = X_test_B  @ w_B\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(train_pos, y_train, label='train target')\n",
    "plt.plot(test_pos, y_test, label='test target')\n",
    "plt.plot(train_pos, y_pred_A_train, linestyle='--', label='PE train pred')\n",
    "plt.plot(test_pos, y_pred_A_test, linestyle='--', label='PE test pred')\n",
    "plt.plot(train_pos, y_pred_B_train, linestyle=':', label='one-hot train pred')\n",
    "plt.plot(test_pos, y_pred_B_test, linestyle=':', label='one-hot test pred')\n",
    "plt.legend()\n",
    "plt.title('Extrapolation: Fixed PE vs One‑Hot Absolute Position')\n",
    "plt.xlabel('position')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "# Short numeric summary (optional)\n",
    "from math import sqrt\n",
    "rmse_A_test = sqrt(np.mean((y_pred_A_test - y_test)**2))\n",
    "rmse_B_test = sqrt(np.mean((y_pred_B_test - y_test)**2))\n",
    "print(f'PE test RMSE:    {rmse_A_test:.4f}')\n",
    "print(f'One-hot RMSE:   {rmse_B_test:.4f}   (expected to be large / degenerate)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c671810",
   "metadata": {},
   "source": [
    "## Part 5 — How PE changes attention (Required)\n",
    "\n",
    "Consider a toy sequence of `L=12` tokens with base token embeddings all equal to zero (for isolation).  \n",
    "We add positional encodings and compute a **self-attention score matrix** using simple **queries/keys** as linear projections of the PE.\n",
    "\n",
    "**Task:**\n",
    "1. Build `Q = PE @ Wq`, `K = PE @ Wk` with small random matrices `Wq, Wk` (seeded).\n",
    "2. Compute raw scores `S = Q K^T / sqrt(d_k)` and visualize `S` (no softmax needed).\n",
    "3. Repeat **without** positional encodings (i.e., zeros only). Compare.\n",
    "\n",
    "**Prompt:** In the Markdown cell, explain what patterns you see and why PE is necessary for attention to distinguish positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a05400",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "L = 12\n",
    "d_model = 32\n",
    "d_k = 16\n",
    "\n",
    "# with PE\n",
    "PE = sinusoidal_pe(L, d_model)\n",
    "Wq = rng.standard_normal((d_model, d_k)) / np.sqrt(d_model)\n",
    "Wk = rng.standard_normal((d_model, d_k)) / np.sqrt(d_model)\n",
    "\n",
    "Q = PE @ Wq\n",
    "K = PE @ Wk\n",
    "S_with = (Q @ K.T) / np.sqrt(d_k)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(S_with, aspect='auto')\n",
    "plt.colorbar(label='score')\n",
    "plt.title('Raw attention scores WITH positional encodings')\n",
    "plt.xlabel('Key position j')\n",
    "plt.ylabel('Query position i')\n",
    "plt.show()\n",
    "\n",
    "# without PE (all-zero embeddings)\n",
    "X = np.zeros((L, d_model))\n",
    "Q0 = X @ Wq\n",
    "K0 = X @ Wk\n",
    "S_without = (Q0 @ K0.T) / np.sqrt(d_k)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(S_without, aspect='auto')\n",
    "plt.colorbar(label='score')\n",
    "plt.title('Raw attention scores WITHOUT positional encodings')\n",
    "plt.xlabel('Key position j')\n",
    "plt.ylabel('Query position i')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a6444",
   "metadata": {},
   "source": [
    "**Your explanation (3–5 sentences):**  \n",
    "*Write here…*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53efd649",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- Completed code cells (no `NotImplementedError`).\n",
    "- Three brief Markdown explanations (Parts 2, 3, and 5).\n",
    "- The plot for the extrapolation experiment (Part 4) if you attempt it.\n",
    "\n",
    "## Grading Rubric (suggested)\n",
    "- Part 1 (Implementation): 35%\n",
    "- Part 2 (Visualization + explanation): 15%\n",
    "- Part 3 (Cosine matrix + explanation): 25%\n",
    "- Part 5 (Attention effect + explanation): 25%\n",
    "- (+5% extra credit) Part 4 extrapolation experiment\n",
    "\n",
    "---\n",
    "\n",
    "### Short-answer prompts you should be able to address\n",
    "- Why mix **multiple frequencies** across dimensions?\n",
    "- How do fixed sinusoidal encodings help with **extrapolation** to longer sequences?\n",
    "- Why do attention scores collapse without positional information when token embeddings are identical?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
