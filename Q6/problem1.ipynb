{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please read the following DO's and DON'Ts carefully:\n",
    "1. Please read each task carefully. Make sure you fill in (with your answer) on any place that says `YOUR CODE HERE`.\n",
    "2. Please `do not` copy any existing notebook cells into the current notebook. Instead, create a new cell by clicking the insert icon (+) on the top-right corner of each cell. Here is a reference to it:\n",
    "![](cell-insert-before-after.png)\n",
    "    - You can add any number of new cells into your jupyter notebbook.\n",
    "    - Make sure your new cells are of type either `Code` or `Markdown`. Please `do not` choose type `Raw.\n",
    "3. Please `do not` delete any existing cell(s) from the notebook.\n",
    "4. Please `do not` change the type of\n",
    "5. Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "    - Alternatively, you can hit the `Validate` button from the `Nbgrader > Assignment List`.\n",
    "6. Once done, please hit the blue `submit` button in the `Nbgrader > Assignment List`.\n",
    "7. Please write your name and collaborators below between the double quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "535c21960d4663d5edac398cb445d087",
     "grade": false,
     "grade_id": "jupyter",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For this problem set, we'll be using the Jupyter notebook:\n",
    "\n",
    "![](jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task A - Regression (10 points)\n",
    "\n",
    "In this task you are given a simple regression dataset located at `dataset/q6_regression.csv`. It has 100 samples. Each sample is 5 dimensional (i.e., having 5 input features namely `x0`, `x1`, `x2`, `x3`, and `x4`). The target `y` is the dependent variable (feature) that you are going to map with the input features. And, you are going to build the following Artificial Neural Network to model it. Also, we are going to use the `PyTorch` compute framework. If this is your first time doing something like this, no worries. I'll walk you through the process. In the next quiz, I'll quiz you on this experience. Does that sound fair?\n",
    "\n",
    "Alrighty then, let's proceed.\n",
    "\n",
    "I hope you are now well aware of what a regression task is:\n",
    "\n",
    "> A regression task is a type of supervised learning problem where the goal is to predict a continuous numerical value (rather than a discrete category) based on the input features.\n",
    "\n",
    "So, in the dataset clearly `y` is such continuous numerical value that your artificial neural network should be able to predict based in given 5 input features: `x0`, `x1`, `x2`, `x3`, and `x4`. \n",
    "\n",
    "Let's work on building simple feedforward artificial neural network:\n",
    "* **Input layer**: taking 5 dim numerical inputs.\n",
    "* **Hidden layer 1**: 10 neurons, Sigmoid activated.\n",
    "* **Hidden layer 2**: 3 neurons, ReLU activated.\n",
    "* **Output layer**: 1 neurons, no activation used as it's a regression task. We don't want to squeesh it or scale it. We want the raw net output. Got it?\n",
    "\n",
    "### Things to note:\n",
    "* We will be importing the `torch.nn.functional` class to be able to call some utility functions, like sigmoid, and other activation functions: `import torch.nn.functional as F`\n",
    "- Now, you can call `F.sigmoid()` for sigmoid activation.[sigmoid documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html)\n",
    "- Likewise, you can call `F.relu()` for the ReLU activation. More info [relu documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.relu.html)\n",
    "\n",
    "* We will be using `torch.nn` class and its subclasses from PyTorch for building the layers of artificial neural network: `import torch.nn as nn`\n",
    "* For this task, we will be calling `nn.Linear()` to build \n",
    "- You may want to call `nn.Linear()` based on the packages impoted below. More info [Linear documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "\n",
    "### Enough said, let's get our hands dirty...\n",
    "* We will first import the packages below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn #used to build neural network \n",
    "import torch.nn.functional as F #used for many utility functions needed in your network\n",
    "\n",
    "#The following package is needed to read data samples from external files and\n",
    "#   prepares, and present (in batches) to PyTorch compute framework during training\n",
    "from torch.utils.data.dataloader import Dataset, DataLoader\n",
    "\n",
    "#You guessed it right. The following is used to split the given dataset into 2 parts:\n",
    "#  One is for training and other for testing/evaluation. As I said in class, you must not train a model\n",
    "#  with the whole given dataset. Set aside a split, which we call a test split so that you can test out\n",
    "#  your model once training finished. It's like you are being prepared for the midterm with a set of training\n",
    "#  problems. You don't expect me to train you with exact same problems I put in the real midterm exam, right?\n",
    "#  So, the problems/splits in your midterm will test how good you learned.\n",
    "#  The artificial neural network should be training in a similar fashion. Got it?\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "#The following is a popular package to draw stuff. You'll see later.\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.utils.data.Dataset` class and `torch.utils.data.DataLoader` class\n",
    "\n",
    "In PyTorch, the `torch.utils.data` module provides the **building blocks** for handling datasets and feeding them efficiently into models during training. The two most important classes here are **`Dataset`** and **`DataLoader`**.\n",
    "\n",
    "\n",
    "#### 1. **`torch.utils.data.Dataset`**\n",
    "\n",
    "* **Purpose:** Represents a dataset (the data itself + how to access it).\n",
    "* **What it does:** Defines **how to get one sample** from your data and **how many samples** the dataset has.\n",
    "* **You usually subclass it** and implement two key methods:\n",
    "\n",
    "  * `__len__(self)` → returns the total number of samples.\n",
    "  * `__getitem__(self, idx)` → retrieves one sample (features + label) by index.\n",
    "\n",
    "#### A quick Example:\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)   # number of samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y\n",
    "```\n",
    "\n",
    "\n",
    "#### 2. **`torch.utils.data.DataLoader`**\n",
    "\n",
    "* **Purpose:** Wraps a `Dataset` and makes it easier to load data **in batches**, **shuffle it**, and even **load in parallel** using multiple worker processes.\n",
    "* **Why useful:** Instead of manually writing loops over the dataset, you just iterate over a `DataLoader`.\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "* `dataset`: the dataset object (must implement `__getitem__` and `__len__`).\n",
    "* `batch_size`: number of samples per batch.\n",
    "* `shuffle`: whether to shuffle data each epoch.\n",
    "* `num_workers`: how many subprocesses to use for data loading (for speed).\n",
    "\n",
    "#### Another quick Example:\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Assume dataset is an instance of MyDataset class above\n",
    "dataset = MyDataset(torch.arange(10), torch.arange(10)*2)\n",
    "\n",
    "# Wrap it in DataLoader\n",
    "loader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "\n",
    "# Iterate over it like this:\n",
    "for batch in loader:\n",
    "    x, y = batch\n",
    "    print(x, y)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Q6_Regression_Dataset` class, which is a subclass of the `Dataset` class\n",
    "* The `Q6_Regression_Dataset` should be able to read from a csv file:\n",
    "   - In the constructor, `__init__` method, you can see we simply read from the dataset. Here, you can pass any data/feature transformation function to work as a preprocessor. We are ignoring that in this task.\n",
    "   - We implemented the `__len___()` member function that simply returns number of samples in the dataset.\n",
    "   - We implemented the `__getitem__()` member function that takes an integer index `idx`, and returns the sample at that index. Here, we are open to return the specific data sample however we want. But, for this task, we are returning the samples as a dictionary so that it's easier to interpret during training. Please note: the values were converted to `torch.tensor` type so that PyTorch can send it to GPU easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q6_Regression_Dataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "                transform (callable, optional): Optional transform to be applied\n",
    "                    on a sample.\n",
    "        \"\"\"\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Example: Assuming last column is the target, all preceding columns are input features.\n",
    "        features = torch.tensor(self.dataframe.iloc[idx, :-1].values, dtype=torch.float32)\n",
    "        label = torch.tensor(self.dataframe.iloc[idx, -1], dtype=torch.float) # or float for regression\n",
    "                                                                              # and long for classification\n",
    "\n",
    "        sample = {'features': features, 'labels': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Q6_Regression_Net` -- let's define the Artificial Neural Network architecture\n",
    "* Remember, we are to define this network:\n",
    "\n",
    "```\n",
    "* **Input layer**: taking 5 dim numerical inputs.\n",
    "* **Hidden layer 1**: 10 neurons, Sigmoid activated.\n",
    "* **Hidden layer 2**: 3 neurons, ReLU activated.\n",
    "* **Output layer**: 1 neurons, no activation\n",
    "```\n",
    "\n",
    "* In PyTorch, neural networks are typically defined by subclassing `nn.Module` class (assuming you imported with `import torch.nn as nn` above.\n",
    "  - You must define the `forward` function that would demonstrate a forward propagation given a data sample, `x`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5303cd5485f87221f37d32c34836683",
     "grade": false,
     "grade_id": "Q6net",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s carefully walk through the **layers** and **activations** defined in the constructor (`__init__`) of the `Q6_Regression_Net` class.\n",
    "\n",
    "### 1. **Layers (`nn.Linear`)**\n",
    "\n",
    "Each **linear layer** in PyTorch performs a transformation of the form:\n",
    "\n",
    "$$\n",
    "y = xW^T + b\n",
    "$$\n",
    "\n",
    "where $W$ are learnable weights and $b$ is a learnable bias.\n",
    "\n",
    "* **`self.fc1 = nn.Linear(5, 10)`**\n",
    "\n",
    "  * Input: 5-dimensional feature vector (since each data sample has 5 features).\n",
    "  * Output: 10-dimensional vector (hidden layer with 10 neurons).\n",
    "\n",
    "* **`self.fc2 = nn.Linear(10, 3)`**\n",
    "\n",
    "  * Takes the 10 outputs from the previous layer.\n",
    "  * Reduces them to 3 outputs (hidden layer with 3 neurons).\n",
    "\n",
    "* **`self.fc3 = nn.Linear(3, 1)`**\n",
    "\n",
    "  * Takes the 3 outputs from hidden layer 2.\n",
    "  * Produces 1 output (final regression value).\n",
    "\n",
    "\n",
    "### 2. **Activations**\n",
    "\n",
    "Activations introduce **non-linearity**, allowing the network to learn (very) complex functions.\n",
    "\n",
    "* **`self.sigmoid = nn.Sigmoid()`**\n",
    "\n",
    "  * Applied after `fc1`.\n",
    "  * Squashes values into the range (0, 1).\n",
    "  * Useful for modeling nonlinear relationships.\n",
    "\n",
    "* **`self.relu = nn.ReLU()`**\n",
    "\n",
    "  * Applied after `fc2`.\n",
    "  * ReLU (Rectified Linear Unit) outputs:\n",
    "\n",
    "    $$\n",
    "    \\text{ReLU}(x) = \\max(0, x)\n",
    "    $$\n",
    "  * Keeps positive values, zeros out negatives.\n",
    "  * Helps with gradient flow and avoids vanishing gradients.\n",
    "\n",
    "* **Final layer (`fc3`) has *no activation***\n",
    "\n",
    "  * Since this is a **regression task**, we want a raw continuous output (could be negative or positive), so we leave it linear.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Forward Pass Summary**\n",
    "\n",
    "When a batch of inputs `x` (shape `[batch_size, 5]`) goes through the network:\n",
    "\n",
    "1. `x → fc1 → sigmoid`\n",
    "   Input (5) → Linear (5→10) → Nonlinear sigmoid → Output (10).\n",
    "2. `x → fc2 → relu`\n",
    "   Input (10) → Linear (10→3) → Nonlinear ReLU → Output (3).\n",
    "3. `x → fc3`\n",
    "   Input (3) → Linear (3→1) → Output (1).\n",
    "\n",
    "Final shape per sample: **1 value** (predicted regression target).\n",
    "\n",
    "Here’s a simple diagram of your regression network:\n",
    "\n",
    "![Q6_Regression_Net.png](Q6_Regression_Net.png)\n",
    "\n",
    "* **Blue circles** → Input layer (5 features)\n",
    "* **Green circles** → Hidden layer 1 (10 neurons, **Sigmoid**)\n",
    "* **Orange circles** → Hidden layer 2 (3 neurons, **ReLU**)\n",
    "* **Red circle** → Output layer (1 neuron, **Linear**)\n",
    "\n",
    "This shows how data flows step by step from inputs → hidden layers → output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's train the network ~ learn the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's instantiate our model\n",
    "model = Q6_Regression_Net()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speaking of loss function. We have plenty to choose from\n",
    "#Reference: https://docs.pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent optimizer, with learning rate,`lr`\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset ready for training\n",
    "full_dataset = Q6_Regression_Dataset(csv_file='dataset/q6_regression.csv')\n",
    "\n",
    "\n",
    "# Define the lengths for the splits (e.g., 80 for training, 20 for validation)\n",
    "train_len = 80\n",
    "test_len = 20\n",
    "split_lengths = [train_len, test_len]\n",
    "\n",
    "# Perform the random split\n",
    "train_dataset, test_dataset = random_split(full_dataset, split_lengths)\n",
    "\n",
    "\n",
    "# Prepare two data loaders to fetch batches of samples from the two datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_dataloader:\n",
    "    print(b['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the training iterations begin below\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for a_batch in train_dataloader:\n",
    "        #Remember? a batch is a dictinary of features & labels\n",
    "        batch_features = a_batch['features']\n",
    "        batch_labels = a_batch['labels']\n",
    "\n",
    "        \n",
    "        # Forward pass the batch\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        #Calculate loss (i.e., error) based on the ground true labels current model outputs\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        # Zero out previously computed gradients... just remember this step before computing new gradients below\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute gradients of the loss with respect to the weights\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust the weights based on the gradient update formula\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now evaluate your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the model into evaluation model. No further \"accidental\" weight adjustments won't happen in this mode.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0 #total MSE score\n",
    "with torch.no_grad():\n",
    "    # Your evaluation code here\n",
    "    for a_batch in test_dataloader:\n",
    "        batch_features = a_batch['features']\n",
    "        batch_labels = a_batch['labels']\n",
    "\n",
    "        # Forward pass the batch\n",
    "        outputs = model(batch_features)\n",
    "\n",
    "        #Calculate & aggregate loss (i.e., error) based on the ground true labels current model outputs\n",
    "        loss += criterion(outputs, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test loss = {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it good enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
